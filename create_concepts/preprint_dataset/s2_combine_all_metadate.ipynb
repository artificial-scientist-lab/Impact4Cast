{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c27b65-fe14-4077-bb51-71bd7aea6e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import linecache\n",
    "import time\n",
    "import jsonlines\n",
    "from datetime import datetime, date\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701b2af-9795-49e0-a4ff-15b9470ac60e",
   "metadata": {},
   "source": [
    "## read biorxiv_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babf3851-6c70-46bd-8626-9b7d6801a3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 184839; Read biorxiv: 184839, Elapsed time: 2.6651909351348877 seconds\n"
     ]
    }
   ],
   "source": [
    "all_paper_full_infos=[] ### store all papers from bioxiv, chem, med, arxiv \n",
    "\n",
    "biorxiv_json = 'biorxiv-metadata-oai-snapshot.json'\n",
    "starting_date = date(1990,1,1)\n",
    "start_time = time.time()\n",
    "\n",
    "with jsonlines.open(biorxiv_json, 'r') as f:\n",
    "    for id_of_abstract, line in enumerate(f):\n",
    "        get_date = datetime.strptime(line['date'], '%Y-%m-%d').date()\n",
    "        paper_time = (get_date - starting_date).days\n",
    "        all_paper_full_infos.append([line['server'],line['title'],line['abstract'],paper_time])\n",
    "\n",
    "num1=len(all_paper_full_infos)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Current: {len(all_paper_full_infos)}; Read biorxiv: {len(all_paper_full_infos)}, Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407d05e-e234-4761-9e46-6a8c9f550cd9",
   "metadata": {},
   "source": [
    "## read medrxiv_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc41bb7-fda8-4029-83ac-844071ee0134",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 224071; Read medrxiv: 39232, Elapsed time: 0.6739270687103271 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "medrxiv_json = 'medrxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "start_time = time.time()\n",
    "with jsonlines.open(medrxiv_json, 'r') as f:\n",
    "    for id_of_abstract, line in enumerate(f):\n",
    "        get_date = datetime.strptime(line['date'], '%Y-%m-%d').date()\n",
    "        paper_time = (get_date - starting_date).days\n",
    "        all_paper_full_infos.append([line['server'],line['title'],line['abstract'],paper_time])\n",
    "\n",
    "num2=len(all_paper_full_infos)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Current: {len(all_paper_full_infos)}; Read medrxiv: {len(all_paper_full_infos)-num1}, Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6151f3-eb2a-4eab-aad3-d50246eddb38",
   "metadata": {},
   "source": [
    "## read chemrxiv_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b19d82-ffff-4c2a-ba37-57031da11cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 240551; Read chemrxiv: 16480, Elapsed time: 0.25910282135009766 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chemrxiv_json = 'chemrxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "start_time = time.time()\n",
    "with jsonlines.open(chemrxiv_json, 'r') as f:\n",
    "    for id_of_abstract, line in enumerate(f):\n",
    "        get_date = datetime.strptime(line['date'][:10], '%Y-%m-%d').date()\n",
    "        paper_time = (get_date - starting_date).days\n",
    "        all_paper_full_infos.append([line['server'],line['title'],line['abstract'],paper_time])\n",
    "\n",
    "num3=len(all_paper_full_infos)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Current: {len(all_paper_full_infos)}; Read chemrxiv: {len(all_paper_full_infos)-num2}, Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b282f2f-561a-4453-b4cd-5240d062b8ee",
   "metadata": {},
   "source": [
    "## check duplicates papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "909861d0-66db-486a-955e-f72987fcfee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicates: 27\n"
     ]
    }
   ],
   "source": [
    "### check whether there are some duplicates papers might due to api re-requesting; \n",
    "duplicates = {}\n",
    "for i, paper_info in enumerate(all_paper_full_infos):\n",
    "    # Convert the sublist to a hashable type (e.g. tuple) to use as a key\n",
    "    key = tuple(sorted(map(str, paper_info)))\n",
    "    if key in duplicates:\n",
    "        duplicates[key].append(i)\n",
    "    else:\n",
    "        duplicates[key] = [i]\n",
    "\n",
    "# Print the duplicates\n",
    "count=0\n",
    "for key, indices in duplicates.items():\n",
    "    if len(indices) > 1:\n",
    "        #print(f\"{len(indices)} instances of {list(key)}\")\n",
    "        #print(f\"Indices: {indices}\")\n",
    "        count+=1\n",
    "        \n",
    "print(f\"remove duplicates: {count}\") ## for len(indices)>2, need to remove 2; so +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0d91f02-5b36-4814-af9d-d67f8f9a8583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove duplicates: 28\n"
     ]
    }
   ],
   "source": [
    "## remove repeated papers (there are some repeated papers)\n",
    "paper_infos_unique = set(map(tuple, all_paper_full_infos)) # convert each sublist to a tuple and create a set\n",
    "all_paper_infos_unique = list(map(list, paper_infos_unique)) # convert each tuple back to a list and create a list\n",
    "\n",
    "print(f\"remove duplicates: {len(all_paper_full_infos)-len(all_paper_infos_unique)}\")\n",
    "\n",
    "with open('all_paper_info_lists_bio_med_chem.pkl', 'wb') as f:\n",
    "    pickle.dump(all_paper_infos_unique, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecc36a-4de1-40de-a22a-152e2efc6488",
   "metadata": {},
   "source": [
    "## read arxiv_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "136d5452-07e0-4d5f-813c-46009fffce65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv: {id_of_abstract}\n",
      "Current: 2444442; Read chemrxiv: 2203891, Elapsed time: 44.237696170806885 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "arxiv_json = 'arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "with jsonlines.open(arxiv_json, 'r') as f:\n",
    "    for id_of_abstract, line in enumerate(f):\n",
    "        get_date = datetime.strptime(line['versions'][0]['created'], '%a, %d %b %Y %H:%M:%S %Z').date()\n",
    "        paper_time = (get_date - starting_date).days\n",
    "        all_paper_infos_unique.append(['arxiv',line['title'],line['abstract'],paper_time])\n",
    "        \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"arxiv: {id_of_abstract}\")\n",
    "print(f\"Current: {len(all_paper_infos_unique)}; Read chemrxiv: {len(all_paper_infos_unique)-num3}, Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345c8dd-8b5c-4a87-8ef3-4529581f2063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_paper_info_lists.pkl', 'wb') as f:\n",
    "    pickle.dump(all_paper_infos_unique, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35d741-6e93-4ae8-955a-6ed028ab964a",
   "metadata": {},
   "source": [
    "## some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b82d00-77b2-4c30-845c-639482c9908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('all_paper_info_lists.pkl', 'rb') as f:\n",
    "    all_paper_infos_test=pickle.load(f)\n",
    "    \n",
    "x_set = set(map(tuple, all_paper_infos_test))\n",
    "y_set = set(map(tuple, all_paper_infos_test2))\n",
    "\n",
    "if x_set == y_set:\n",
    "    print(\"The lists have the same sublists\")\n",
    "else:\n",
    "    print(\"The lists have different sublists\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
